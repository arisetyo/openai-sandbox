'''
An example of using OpenAI chat endpoint to ask a question to the AI.
This script uses the `completions.create` method from the `chat` endpoint.
The `completions.create` method is used to generate completions based on conversation history.

The model model used in this script is `gpt-3.5-turbo`.
The system message tells the model that it is a "helpful assistant knowledgeable in geography and world history."

@see: https://beta.openai.com/docs/api-reference/completions/create
Note: this is a deprecated endpoint
'''

import os
import argparse
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables from .env file
load_dotenv()

# create openAI client
client = OpenAI(
  organization=os.getenv('OPENAI_ORGANIZATION'),
  project=os.getenv('OPENAI_PROJECT_NAME'), # project name: "Belajar OpenAI"
  api_key=os.getenv('OPENAI_API_KEY')
)

# Set up argument parser
parser = argparse.ArgumentParser(description='Process some parameters.')
parser.add_argument('--ask', type=str, required=True, help='Ask something to the AI')
# Parse arguments
args = parser.parse_args()
# Assign the user request to the variable
user_request = args.ask

# get completion from OpenAI
# we are going to use the `completions.create` method
# this is part of the `chat` endpoint
# the way this method works is that it takes a model, and a list of messages
# the messages are the conversation history
# the model is the model that will be used to generate the completion
# the completion is generated based on the conversation history
# the conversation history is a list of messages
# each message has a role and content
# the role can be "system" or "user"
# role "system" is used for messages generated by the system
# role "user" is used for messages generated by the user

# other methods in the `chat` method include:
# - `chat.search` for searching chat logs
# - `chat.messages.list` for listing messages
# - `chat.messages.create` for creating messages
# - `chat.messages.delete` for deleting messages
completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    # tell the model that it is a helpful assistant knowledgeable in geography and world history
    {
      "role": "system",
      "content": "You are a helpful assistant knowledgeable in geography and world history."
    },
    # tell the model that the user is asking a question
    {
      "role": "user",
      "content": user_request
    },
  ]
)

# print the completion
# here we wait for the whole completion to be generated, instead of streaming it in chunks
output = completion.choices[0].message.content
print(output)

# Example usage:
# python 1-chat.py --ask "List the largest cities in the world by population."